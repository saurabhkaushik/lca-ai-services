{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import json \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from datasets import Dataset\n",
    "from sklearn import preprocessing\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "training_file = \"./cuad-data/test_classification_data.json\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_y = dict()\n",
    "def load_data():\n",
    "    with open(training_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    dataset = data[\"train\"]\n",
    "    return dataset\n",
    "\n",
    "def process_data(row, ):\n",
    "    text = row['sentence']\n",
    "    text = str(text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    encodings = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    label = label_y[row['label']]\n",
    "\n",
    "    encodings['label'] = label\n",
    "    encodings['text'] = text\n",
    "\n",
    "    return encodings\n",
    "\n",
    "def prepare_train_valid_df(): \n",
    "    processed_data = []    \n",
    "    train_data = load_data()\n",
    "    \n",
    "    label_count = 0\n",
    "    for i in range(len(train_data[:1000])):        \n",
    "        key = train_data[i][\"label\"].lower().strip()\n",
    "        if not key in label_y.keys():\n",
    "            label_y.update({key : label_count})\n",
    "            label_count += 1\n",
    "        processed_data.append(process_data(train_data[i]))\n",
    "        \n",
    "    print (\">>>>>> label_y : \", label_y)\n",
    "    print (processed_data)\n",
    "    new_df = pd.DataFrame(processed_data)\n",
    "\n",
    "    train_df, valid_df = train_test_split(\n",
    "        new_df,\n",
    "        test_size=0.2,\n",
    "        random_state=2022\n",
    "    )\n",
    "\n",
    "    train_hg = Dataset(pa.Table.from_pandas(train_df))\n",
    "    valid_hg = Dataset(pa.Table.from_pandas(valid_df))\n",
    "    return train_hg, valid_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_hg, valid_hg):\n",
    "    training_args = TrainingArguments(output_dir=\"./result\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "    id2label = {0: \"negative\", 1: \"positive\"}\n",
    "    label2id = {val: key for key, val in id2label.items()}\n",
    "    num_labels = len(id2label)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id)  \n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_hg,\n",
    "        eval_dataset=valid_hg,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    model.save_pretrained('./model/')\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(sentences, model): \n",
    "    #model = AutoModelForSequenceClassification.from_pretrained('./model/')\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    results = classifier(sentences)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(process_data({\n",
    "    'sentence': 'this is a sample review of a movie.',\n",
    "    'label': 1\n",
    "}))\n",
    "\n",
    "train_hg, valid_hg = prepare_train_valid_df()\n",
    "model = training(train_hg, valid_hg) \n",
    "print (train_hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>> label_y :  {'positive': 0, 'negative': 1}\n",
      "[{'input_ids': [101, 5062, 1010, 13202, 1998, 9949, 3615, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'text': 'Broadcasting, Licensing and Wireless referred to'}, {'input_ids': [101, 13202, 1998, 9949, 2024, 3615, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'text': 'Licensing and Wireless are referred to'}, {'input_ids': [101, 5062, 1010, 13202, 1998, 2024, 3615, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'text': 'Broadcasting, Licensing and are referred to'}, {'input_ids': [101, 5062, 1010, 13202, 1998, 9949, 2024, 3615, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'text': 'Broadcasting, Licensing and Wireless are referred to'}, {'input_ids': [101, 5062, 1010, 13202, 1998, 9949, 2024, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'text': 'Broadcasting, Licensing and Wireless are to'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/saurabhkaushik/Workspace/lca-ai-services/env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 1/3 [00:01<00:02,  1.11s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "                                             \n",
      " 33%|███▎      | 1/3 [00:01<00:02,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6975694894790649, 'eval_runtime': 0.1423, 'eval_samples_per_second': 7.029, 'eval_steps_per_second': 7.029, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.45s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "                                             \n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6794772148132324, 'eval_runtime': 0.162, 'eval_samples_per_second': 6.174, 'eval_steps_per_second': 6.174, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.44s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "                                             \n",
      "100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.45s/it]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6756847500801086, 'eval_runtime': 0.0862, 'eval_samples_per_second': 11.597, 'eval_steps_per_second': 11.597, 'epoch': 3.0}\n",
      "{'train_runtime': 4.4056, 'train_samples_per_second': 2.724, 'train_steps_per_second': 0.681, 'train_loss': 0.6671514511108398, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 280.57it/s]\n",
      "Configuration saved in ./model/config.json\n",
      "Model weights saved in ./model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'score': 0.5360666513442993}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    {\"sentence\" : \"Licensing and Wireless are referred to \"},\n",
    "    {\"sentence\" : \"The judge told that the jurors to think carefully.\"}\n",
    "]\n",
    "\n",
    "train_hg, valid_hg = prepare_train_valid_df()\n",
    "model = training(train_hg, valid_hg) \n",
    "results = predict(sentences[0][\"sentence\"], model)\n",
    "\n",
    "print (results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0-87': {'start_index': 0, 'end_index': 87, 'relevence': 52.38409638404846, 'relevence_degree': 'MEDIUM'}, '88-113': {'start_index': 88, 'end_index': 113, 'relevence': 53.44003438949585, 'relevence_degree': 'MEDIUM'}, '479-689': {'start_index': 479, 'end_index': 689, 'relevence': 50.180864334106445, 'relevence_degree': 'MEDIUM'}, '690-734': {'start_index': 690, 'end_index': 734, 'relevence': 52.11862921714783, 'relevence_degree': 'MEDIUM'}, '0-0': {'start_index': 0, 'end_index': 0, 'relevence': 52.51891613006592, 'relevence_degree': 'MEDIUM'}}\n"
     ]
    }
   ],
   "source": [
    "test_data_file = './cuad-data/test_contract.json'\n",
    "contract_terms_file = './cuad-data/contract_terms.json'\n",
    "import re\n",
    "\n",
    "def prep_data():\n",
    "    with open(test_data_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    contract = data['contract']\n",
    "    return contract\n",
    "\n",
    "def process_paragraph(article_text, model):\n",
    "    return_value = {}\n",
    "    catch_stmt = {}\n",
    "    for c_sentence in article_text.split('.'):\n",
    "        results = predict(c_sentence, model)\n",
    "        score = (results[0][\"score\"]  * 100)\n",
    "        try: \n",
    "            #print (\">>>>>>>>  Found Sentence 1 : \", catch_stmt[results[0][\"label\"]])\n",
    "            res = re.search(c_sentence, article_text) # TODO Revisite \n",
    "            #print (\">>>>>> Found Sentences 2 - \", res)\n",
    "            if res:\n",
    "                #print(\">>>>>>>> Index : \", res.start(), res.end())\n",
    "                #print(\">>>>>>>>  Input String : \", c_sentence)   \n",
    "                #print(\">>>>>>>>  Output String : \", article_text[res.start() : res.end()])   \n",
    "                stmt_index = str(res.start()) + \"-\" + str(res.end())\n",
    "                relevence = score\n",
    "                return_value[stmt_index] = {\"start_index\" : res.start(), \"end_index\" : res.end(), \"relevence\" : relevence}\n",
    "        except IndexError:\n",
    "            #print (\">>>>>>>> Word Not Found\")\n",
    "            print()\n",
    "    #print (\"return_value : \\n\", return_value)\n",
    "\n",
    "    return return_value\n",
    "\n",
    "def highlight_ranking(return_value):\n",
    "    for r_key in return_value:\n",
    "        score = return_value[r_key]['relevence'] \n",
    "        #print(return_value[r_key]['relevence'] , \" >> \" , score)\n",
    "        if score > 67: \n",
    "            return_value[r_key][\"relevence_degree\"] = \"HIGH\"\n",
    "        else: \n",
    "            if score > 34: \n",
    "                return_value[r_key][\"relevence_degree\"] = \"MEDIUM\"\n",
    "            else:\n",
    "                if score > 0: \n",
    "                    return_value[r_key][\"relevence_degree\"] = \"LOW\"\n",
    "\n",
    "    #print (\"return_value : \\n\", return_value)\n",
    "    return return_value\n",
    "\n",
    "article_text = prep_data()\n",
    "return_value = process_paragraph(article_text, model)\n",
    "return_value = highlight_ranking(return_value)\n",
    "print (return_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e5532c3338948ef3923f84fcb49a3fec3185e68b18b7cb85088cd68c49cbe22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
