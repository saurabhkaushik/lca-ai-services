{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_data_file = './testing/test_data-2.json'\n",
    "\n",
    "def prep_data():\n",
    "    with open(test_data_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    contract = data['liabilities']\n",
    "    return contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "contracts = prep_data()\n",
    "sent_list = []\n",
    "\n",
    "for txt in contracts: \n",
    "    doc = nlp(txt['data'])\n",
    "    print('Text: ', txt['data'])\n",
    "    data_dict = {}\n",
    "    for ent in doc.ents:        \n",
    "        #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        if ent.label_ == 'MONEY':\n",
    "            data_dict['money'] = ent.text\n",
    "        if ent.label_ == 'DATE':\n",
    "            data_dict['date'] = ent.text\n",
    "\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"VERB\"): # and (not token.is_stop):\n",
    "            data_dict['verb'] = token.text\n",
    "    sent_list.append(data_dict)\n",
    "    print (data_dict)\n",
    "\n",
    "print (sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "contracts = prep_data()\n",
    "sent_list = []\n",
    "\n",
    "for txt in contracts: \n",
    "    doc = nlp(txt['data'])\n",
    "    print('Text: ', txt['data'])\n",
    "    displacy.render(doc,jupyter=True)\n",
    "\n",
    "    for token in doc:\n",
    "        print(token.text, token.dep_, token.head.text, [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': '', 'description': ''}, {'aspect': 'company', 'description': 'short'}, {'aspect': '', 'description': 'accrued'}, {'aspect': '', 'description': 'payable'}, {'aspect': 'liabilities', 'description': 'same'}, {'aspect': '', 'description': ''}, {'aspect': 'company', 'description': 'certain'}]\n",
      "[{'aspect': '', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'company', 'description': 'short', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.3)}, {'aspect': '', 'description': 'accrued', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': '', 'description': 'payable', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'liabilities', 'description': 'same', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.125)}, {'aspect': '', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'company', 'description': 'certain', 'sentiment': Sentiment(polarity=0.21428571428571427, subjectivity=0.5714285714285714)}]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "contracts = prep_data()\n",
    "\n",
    "aspects = []\n",
    "for txt in contracts: \n",
    "  doc = nlp(txt['data'])\n",
    "  descriptive_term = ''\n",
    "  target = ''\n",
    "  for token in doc:\n",
    "    if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "      target = token.text\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  aspects.append({'aspect': target,\n",
    "    'description': descriptive_term})\n",
    "print(aspects)\n",
    "\n",
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "  aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Based NER Model - Choosen Spacy over this. \n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "#model_path = 'cuad-models/roberta-base/'\n",
    "model_path = \"dslim/bert-base-NER\"\n",
    "#model_path = \"distilbert-base-uncased\"\n",
    "\n",
    "def initialise(): \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    question=\"null\"\n",
    "    tokenizer.encode(question, truncation=True, padding=True)\n",
    "\n",
    "    nlp = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "    return tokenizer, nlp\n",
    "\n",
    "def predict(contract, tokenizer, nlp):\n",
    "    tokenizer.encode(contract, truncation=True, padding=True)\n",
    "    ner_result = nlp(contract)\n",
    "\n",
    "    return ner_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, nlp = initialise()\n",
    "contracts = prep_data()\n",
    "\n",
    "for txt in contracts: \n",
    "    answer = predict(txt['data'], tokenizer, nlp)\n",
    "    print('Text: ', txt['data'])\n",
    "    print('NER: ', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"\"\"Compatibility of systems of linear constraints over the set of natural numbers.\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict inequations,\n",
    "and nonstrict inequations are considered. Upper bounds for components of a minimal set of\n",
    "solutions and algorithms of construction of minimal generating sets of solutions for all types\n",
    "of systems are given. These criteria and the corresponding algorithms for constructing a minimal\n",
    "supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\"\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "doc = nlp(text)\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from app.PreProcessText import PreProcessText\n",
    "\n",
    "ppt = PreProcessText()\n",
    "\n",
    "def find_dep_key(text, phrase):\n",
    "    if not text:\n",
    "        return None\n",
    "    text = ppt.get_lemmantizer(text)\n",
    "    phrase = ppt.get_lemmantizer(phrase)\n",
    "    # search phrase in text \n",
    "    found_text = re.search(phrase, text)\n",
    "    if found_text:\n",
    "        return found_text.group(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saurabhkaushik/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Decimal('5.0'), 'USD'), (Decimal('1000000.0'), 'GBP')]\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install lexnlp \n",
    "import nltk \n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000000000.0\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.money\n",
    "text = \"The price will be  $4.439 billion GBP.\"\n",
    "nlist = list(lexnlp.extract.en.money.get_money(text))\n",
    "print (nlist[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "num = 545454\n",
    "x = len(str(num))\n",
    "print (pow(10, x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e5532c3338948ef3923f84fcb49a3fec3185e68b18b7cb85088cd68c49cbe22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
